#!/usr/bin/env python3
"""
ğŸŒ WEB SEARCH SERVICE - Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ
"""

import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
import random
from config import WEB_SEARCH_TIMEOUT, WEB_SEARCH_RESULTS
from logger import get_logger

logger = get_logger(__name__)


class WebSearchService:
    """ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ"""

    def __init__(self):
        logger.info("âœ… WebSearchService Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°")
        self.timeout = aiohttp.ClientTimeout(total=WEB_SEARCH_TIMEOUT)
        self.max_results = WEB_SEARCH_RESULTS
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
        ]

    def _get_headers(self):
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'DNT': '1',
        }

    async def search(self, query: str, num_results: int = 5) -> list:
        """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¸ÑĞºĞ°"""
        try:
            logger.info(f"ğŸ” Web Ğ¿Ğ¾Ğ¸ÑĞº: '{query[:50]}'...")

            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ DuckDuckGo Lite (ÑĞ°Ğ¼Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹)
            results = await self._search_ddg_lite(query)
            if results:
                logger.info(f"âœ… DuckDuckGo: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(results)}")
                return results[:num_results]

            # Fallback: Bing
            logger.debug("ğŸ” Fallback Ğ½Ğ° Bing...")
            results = await self._search_bing(query)
            if results:
                logger.info(f"âœ… Bing: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(results)}")
                return results[:num_results]

            # Fallback: Google
            logger.debug("ğŸ” Fallback Ğ½Ğ° Google...")
            results = await self._search_google(query)
            if results:
                logger.info(f"âœ… Google: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(results)}")
                return results[:num_results]

            logger.warning(f"âš ï¸ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
            return []

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
            return []

    async def _search_ddg_lite(self, query: str) -> list:
        """DuckDuckGo Lite - ÑĞ°Ğ¼Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹"""
        try:
            url = "https://lite.duckduckgo.com/lite/"
            data = {'q': query}
            
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.post(url, data=data, headers=self._get_headers()) as response:
                    if response.status != 200:
                        return []

                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    for row in soup.find_all('tr')[1:]:  # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
                        try:
                            cells = row.find_all('td')
                            if len(cells) >= 2:
                                link = cells[0].find('a')
                                if link:
                                    title = link.text.strip()
                                    url = link.get('href', '')
                                    snippet = cells[1].text.strip() if len(cells) > 1 else ''
                                    
                                    if title and url and len(snippet) > 20:
                                        results.append({
                                            'title': title,
                                            'url': url,
                                            'snippet': snippet
                                        })
                        except:
                            continue

                    return results[:self.max_results]

        except Exception as e:
            logger.debug(f"ğŸ” DuckDuckGo: {e}")
            return []

    async def _search_bing(self, query: str) -> list:
        """Bing Ğ¿Ğ¾Ğ¸ÑĞº"""
        try:
            url = f"https://www.bing.com/search?q={quote_plus(query)}"
            
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, headers=self._get_headers()) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    for item in soup.select('li.b_algo')[:self.max_results]:
                        try:
                            h2 = item.find('h2')
                            link = h2.find('a') if h2 else None
                            snippet = item.find('p')
                            
                            if link and snippet:
                                results.append({
                                    'title': link.text.strip(),
                                    'url': link.get('href', ''),
                                    'snippet': snippet.text.strip()
                                })
                        except:
                            continue

                    return results

        except Exception as e:
            logger.debug(f"ğŸ” Bing: {e}")
            return []

    async def _search_google(self, query: str) -> list:
        """Google Ğ¿Ğ¾Ğ¸ÑĞº"""
        try:
            url = f"https://www.google.com/search?q={quote_plus(query)}"
            
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, headers=self._get_headers()) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    for g in soup.select('div.g')[:self.max_results]:
                        try:
                            link = g.find('a')
                            h3 = g.find('h3')
                            
                            if link and h3:
                                snippet_div = g.select_one('div.s, span.st')
                                snippet = snippet_div.text.strip() if snippet_div else ''
                                
                                if snippet and len(snippet) > 20:
                                    results.append({
                                        'title': h3.text.strip(),
                                        'url': link.get('href', ''),
                                        'snippet': snippet
                                    })
                        except:
                            continue

                    return results

        except Exception as e:
            logger.debug(f"ğŸ” Google: {e}")
            return []