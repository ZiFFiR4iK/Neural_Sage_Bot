#!/usr/bin/env python3
"""
ü¶ô core/llm_service.py
"""

import re
import requests
from config import (
    OLLAMA_HOST,
    LLM_MODEL,
    LLM_TEMPERATURE,
    LLM_TOP_K,
    REQUEST_TIMEOUT,
    MODE_CONFIGS,
)
from logger import get_logger

logger = get_logger(__name__)


class LLMService:
    """LLM —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∂–∏–º–æ–≤ –∏ –∞–≤—Ç–æ-–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞."""

    SYSTEM_PROMPT = """–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ AI –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

    –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û: –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.

    –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç - –ø–µ—Ä–µ–≤–æ–¥–∏ –µ–≥–æ –∏ –æ—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏.
    –ï—Å–ª–∏ –Ω—É–∂–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º - –æ—Å—Ç–∞–≤–ª—è–π –∫–æ–¥ –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

    –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
    1. –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
    2. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –¥–∞–Ω
    3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    4. –ï—Å–ª–∏ –ø—Ä–∏–≤–æ–¥–∏—à—å –∫–æ–¥ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—ã - –æ–±–æ—Ä–∞—á–∏–≤–∞–π –≤ ```python``` –∏–ª–∏ ```bash```
    5. –ë—É–¥—å –≤–µ–∂–ª–∏–≤ –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–µ–Ω
    6. –ö–†–ò–¢–ò–ß–ù–û: –æ—Ç–≤–µ—Ç –ü–û–õ–ù–û–°–¢–¨–Æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ!

    –Ø–∑—ã–∫: –†–£–°–°–ö–ò–ô (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)"""

    INCOMPLETE_THRESHOLD = 0.85
    MAX_CONTINUATION_RETRIES = 2

    def __init__(self):
        self.host = OLLAMA_HOST
        self.model = LLM_MODEL
        self.endpoint = f"{self.host}/api/generate"
        logger.info(f"‚úÖ LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.model} @ {self.host}")

    def _clean_answer(self, text: str) -> str:
        """–õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏."""
        if not text:
            return ""

        text = re.sub(r"\n\n\n+", "\n\n", text)
        text = "\n".join(line.rstrip() for line in text.split("\n"))
        return text.strip()

    def _looks_incomplete(self, text: str, max_tokens: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—ã–≥–ª—è–¥–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º."""
        if not text:
            return False

        tail = text.strip()[-40:]
        proper_endings = (".", "!", "?", "```", "`", '"', "¬ª", ")", "]", "}")

        if any(tail.endswith(e) for e in proper_endings):
            return False

        est_tokens = len(text) / 3.5
        ratio = est_tokens / max_tokens if max_tokens > 0 else 0.0

        if ratio > self.INCOMPLETE_THRESHOLD:
            logger.warning(
                f"‚ö†Ô∏è –û—Ç–≤–µ—Ç –≤—ã–≥–ª—è–¥–∏—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º ({ratio*100:.1f}% –æ—Ç –ª–∏–º–∏—Ç–∞)"
            )
            return True

        return False

    def _call_ollama(
        self,
        full_prompt: str,
        max_tokens: int,
        temperature: float,
        top_k: int,
    ) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –≤—ã–∑–æ–≤ –∫ Ollama API."""
        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": False,
            "num_predict": max_tokens,
            "temperature": temperature,
            "top_k": top_k,
        }

        logger.debug(
            f"üîç DEBUG: LLM –∑–∞–ø—Ä–æ—Å (—Ç–æ–∫–µ–Ω–æ–≤={max_tokens}, temp={temperature}, top_k={top_k})"
        )

        try:
            response = requests.post(
                self.endpoint,
                json=payload,
                timeout=REQUEST_TIMEOUT,
            )

            response.raise_for_status()
            result = response.json()
            answer = (result.get("response") or "").strip()

            if answer:
                logger.info(
                    f"‚úÖ LLM –æ—Ç–≤–µ—Ç–∏–ª–∞ ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)"
                )
            else:
                logger.warning("‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π response")

            return answer

        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout: Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
            return ""
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå ConnectionError: Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")
            return ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
            return ""

    def generate(self, prompt: str, context: str = "", mode: str = "default") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–µ–∂–∏–º–∞.

        Args:
            prompt: –æ—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
            context: –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∏–∑ –ë–î –∏–ª–∏ –≤–µ–±–∞)
            mode: "short", "default" –∏–ª–∏ "detailed"

        Returns:
            str: –æ—Ç–≤–µ—Ç –æ—Ç LLM
        """
        try:
            if mode not in MODE_CONFIGS:
                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º '{mode}', –∏—Å–ø–æ–ª—å–∑—É–µ–º 'default'")
                mode = "default"

            cfg = MODE_CONFIGS.get(mode, {})
            max_tokens = int(cfg.get("num_predict", 600))
            top_k = int(cfg.get("top_k", LLM_TOP_K))
            temperature = float(cfg.get("temperature", LLM_TEMPERATURE))

            if mode == "short":
                max_tokens = min(max_tokens, 120)

            full_prompt = f"{self.SYSTEM_PROMPT}\n\n{prompt}"

            answer = self._call_ollama(
                full_prompt=full_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_k=top_k,
            )

            if not answer:
                return ""

            answer = self._clean_answer(answer)

            # Continuation –¥–ª—è DEFAULT –∏ DETAILED, –Ω–æ –ù–ï –¥–ª—è SHORT
            enable_continuation = mode in ["default", "detailed"]

            retries = 0
            while (
                enable_continuation
                and self._looks_incomplete(answer, max_tokens)
                and retries < self.MAX_CONTINUATION_RETRIES
            ):
                retries += 1
                logger.info(
                    f"üîÑ Continuation {retries}/{self.MAX_CONTINUATION_RETRIES} (—Ä–µ–∂–∏–º {mode})"
                )
                
                continuation_prompt = (
                    f"{self.SYSTEM_PROMPT}\n\n"
                    f"[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç. "
                    f"–ü—Ä–æ–¥–æ–ª–∂–∏ –æ—Ç–≤–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π. "
                    f"–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —É–∂–µ –∑–∞–∫–æ–Ω—á–µ–Ω –ª–æ–≥–∏—á–Ω–æ, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏ —á—Ç–æ –æ–Ω –ø–æ–ª–Ω—ã–π.]\n\n"
                    f"–ü–†–ï–î–´–î–£–©–ò–ô –û–¢–í–ï–¢ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—Ä–µ–∑–∞–Ω):\n{answer}"
                )
                
                continuation = self._call_ollama(
                    full_prompt=continuation_prompt,
                    max_tokens=max_tokens // 2,
                    temperature=temperature,
                    top_k=top_k,
                )
                
                if not continuation:
                    logger.warning("‚ö†Ô∏è Continuation –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    break
                
                continuation = self._clean_answer(continuation)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ continuation –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                if continuation.lower() in answer.lower() or len(continuation) < 50:
                    logger.info("‚ÑπÔ∏è Continuation –ø–æ–≤—Ç–æ—Ä–∏–ª —Ç–µ–∫—Å—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, —Å—Ç–æ–ø–∏–º")
                    break
                
                answer = f"{answer}\n\n{continuation}"
                logger.info(f"‚úÖ Continuation: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(continuation)} —Å–∏–º–≤–æ–ª–æ–≤")

            if retries > 0:
                logger.info(
                    f"‚úÖ –û—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤ ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤, —Ä–µ–∂–∏–º {mode})"
                )

            return answer

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM.generate: {e}")
            return ""