#!/usr/bin/env python3
"""
ğŸ”§ config.py - Ğ”Ğ˜ĞĞĞœĞ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞšĞĞĞ¤Ğ˜Ğ“ Ğ¡ .env
"""

import os
import json
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“‚ PATHS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PROJECT_ROOT = Path(__file__).parent
LOGS_PATH = PROJECT_ROOT / "logs"
LOGS_PATH.mkdir(exist_ok=True)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ¤– TELEGRAM
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TELEGRAM_TOKEN:
    raise ValueError("âŒ TELEGRAM_TOKEN Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² .env!")

TELEGRAM_BOT_TOKEN = TELEGRAM_TOKEN
TELEGRAM_BOT_DEBUG = os.getenv("DEBUG", "false").lower() == "true"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ¦™ OLLAMA / LLM (Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
LLM_MODEL = os.getenv("LLM_MODEL", "neural-chat")
LLM_NUM_PREDICT = int(os.getenv("LLM_NUM_PREDICT", "2000"))
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.3"))
LLM_TOP_K = int(os.getenv("LLM_TOP_K", "3"))

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ¯ Ğ Ğ•Ğ–Ğ˜ĞœĞ« - Ğ”Ğ˜ĞĞĞœĞ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞšĞĞĞ¤Ğ˜Ğ“ Ğ˜Ğ— .env
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _parse_mode_config(env_var: str, default: dict) -> dict:
    """ĞŸĞ°Ñ€ÑĞ¸Ñ‚ÑŒ JSON ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ¸Ğ· .env"""
    value = os.getenv(env_var)
    if not value:
        return default
    
    try:
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸ ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ
        value = value.strip().strip("'\"")
        parsed = json.loads(value)
        return {**default, **parsed}
    except json.JSONDecodeError as e:
        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° {env_var}: {e}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚")
        print(f"   Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: {value}")
        return default


_DEFAULT_SHORT = {
    "num_predict": 300,
    "temperature": 0.2,
    "top_k": 2,
    "db_search": True,      
    "web_search": True,     
    "web_search_results": 2,  
}

_DEFAULT_DEFAULT = {
    "num_predict": 1000,
    "temperature": 0.5,
    "top_k": 3,
    "db_search": True,      
    "web_search": True,     
    "web_search_results": 3,
}

_DEFAULT_DETAILED = {
    "num_predict": 2000,
    "temperature": 0.7,
    "top_k": 5,
    "db_search": True,      
    "web_search": True,     
    "web_search_results": 5,  
}

MODE_CONFIGS = {
    "short": _parse_mode_config("MODE_SHORT", _DEFAULT_SHORT),
    "default": _parse_mode_config("MODE_DEFAULT", _DEFAULT_DEFAULT),
    "detailed": _parse_mode_config("MODE_DETAILED", _DEFAULT_DETAILED),
}

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸŒ WEB SEARCH
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WEB_SEARCH_TIMEOUT = int(os.getenv("WEB_SEARCH_TIMEOUT", "8"))
WEB_SEARCH_RESULTS = int(os.getenv("WEB_SEARCH_RESULTS", "5"))

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“Š LOGGING
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"
LOG_FILE = LOGS_PATH / "bot.log"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# â±ï¸ TIMEOUTS & RETRIES
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "30"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))
QUEUE_FILE = PROJECT_ROOT / "pending_messages.json"
QUEUE_AUTO_RETRY = os.getenv("QUEUE_AUTO_RETRY", "true").lower() == "true"
QUEUE_RETRY_DELAY = int(os.getenv("QUEUE_RETRY_DELAY", "300"))
TELEGRAM_RETRY_ATTEMPTS = int(os.getenv("TELEGRAM_RETRY_ATTEMPTS", "3"))
TELEGRAM_RETRY_DELAY = int(os.getenv("TELEGRAM_RETRY_DELAY", "2"))

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“š CHROMA / DATABASE
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CHROMA_PATH = os.getenv("CHROMA_PATH", "chroma_db")
CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "documents")
CHROMA_SEARCH_TOPK = int(os.getenv("CHROMA_SEARCH_TOPK", "5"))
CHROMA_SIMILARITY_THRESHOLD = float(os.getenv("CHROMA_SIMILARITY_THRESHOLD", "0.6"))
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "10"))
EMBEDDING_CACHE_SIZE = int(os.getenv("EMBEDDING_CACHE_SIZE", "1000"))

DB_AUTO_ADD_SOURCES = os.getenv("DB_AUTO_ADD_SOURCES", "true").lower() == "true"
DB_CLEANUP_DAYS = int(os.getenv("DB_CLEANUP_DAYS", "60"))
DB_AUTO_CLEANUP = os.getenv("DB_AUTO_CLEANUP", "false").lower() == "true"

print(f"âœ… CHROMA: path={CHROMA_PATH}, collection={CHROMA_COLLECTION_NAME}, topk={CHROMA_SEARCH_TOPK}")
print(f"âœ… EMBEDDINGS: model={EMBEDDING_MODEL}, batch_size={EMBEDDING_BATCH_SIZE}")
print(f"âœ… DB: similarity_threshold={CHROMA_SIMILARITY_THRESHOLD}, auto_add={DB_AUTO_ADD_SOURCES}")
print(f"âœ… EMBEDDING CACHE: size={EMBEDDING_CACHE_SIZE}")


def print_config():
    """Ğ’Ñ‹Ğ²ĞµÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ"""
    print("\n" + "=" * 70)
    print("ğŸ¤– WEB-FIRST RAG BOT - ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯")
    print("=" * 70)
    print(f"ğŸ¦™ LLM: {LLM_MODEL} @ {OLLAMA_HOST}")
    print(f"ğŸ“Š Web Search: Timeout={WEB_SEARCH_TIMEOUT}s, Results={WEB_SEARCH_RESULTS}")
    print(f"ğŸŒ¡ï¸ LLM Baseline: Temp={LLM_TEMPERATURE}, Top-K={LLM_TOP_K}")

    if TELEGRAM_TOKEN:
        print(f"âœ… Telegram Token: {'***' + TELEGRAM_TOKEN[-6:]}")
    else:
        print("âŒ Telegram Token: ĞĞ• Ğ£Ğ¡Ğ¢ĞĞĞĞ’Ğ›Ğ•Ğ!")

    print("\nğŸ“‹ Ğ Ğ•Ğ–Ğ˜ĞœĞ« (Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· .env):")
    for mode_name, config in MODE_CONFIGS.items():
        print(f"\n [{mode_name.upper()}]")
        print(f" â€¢ ĞœĞ°ĞºÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: {config['num_predict']}")
        print(f" â€¢ Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°: {config['temperature']}")
        print(f" â€¢ Top-K: {config['top_k']}")
        print(f" â€¢ ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ‘Ğ”: {'âœ…' if config.get('db_search') else 'âŒ'}")
        print(f" â€¢ Ğ’ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº: {'âœ…' if config.get('web_search') else 'âŒ'}")

    print("=" * 70 + "\n")